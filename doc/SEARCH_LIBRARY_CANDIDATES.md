# OCR結果検索用ローカルライブラリ候補

## 要件

- **ローカル前提**: ローカルで構築可能（Docker可）
- **エミュレート可能**: クラウドサービスでも構築可能
- **OCR結果の検索**: 抽出されたテキストを検索対象
- **LLM連携**: 自然言語による検索を想定（ベクトル検索が有利）
- **Go統合**: Go言語からの利用を想定

## 候補ライブラリ

### 1. Elasticsearch

**概要**: 分散検索・分析エンジン

**メリット**:
- 強力な全文検索機能
- 豊富なクエリ機能（fuzzy, phrase, range等）
- エコシステムが充実
- Dockerでのローカル構築が容易
- Kibanaによる可視化

**デメリット**:
- リソース消費が大きい（メモリ2GB以上推奨）
- 設定が複雑
- ベクトル検索は有料版（Elasticsearch 8.0+）

**ローカル構築**:
```yaml
# docker-compose.yml
elasticsearch:
  image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
  environment:
    - discovery.type=single-node
    - xpack.security.enabled=false
  ports:
    - "9200:9200"
```

**LLM連携**: 
- ベクトル検索は有料版が必要
- 全文検索のみなら無料で使用可能

**評価**: ⭐⭐⭐⭐ (全文検索なら優秀、ベクトル検索は有料)

---

### 2. Meilisearch

**概要**: 高速・軽量な検索エンジン（Rust製）

**メリット**:
- 非常に軽量（Dockerイメージ ~100MB）
- 高速な全文検索
- タイポ許容検索（typo-tolerant）
- ファセット検索対応
- REST APIがシンプル
- オープンソース（MIT）

**デメリット**:
- ベクトル検索は未対応（2024年11月時点）
- 分散構成はEnterprise版
- 分析機能は限定的

**ローカル構築**:
```yaml
# docker-compose.yml
meilisearch:
  image: getmeili/meilisearch:latest
  ports:
    - "7700:7700"
  environment:
    - MEILI_MASTER_KEY=masterKey
  volumes:
    - meili_data:/meili_data
```

**LLM連携**: 
- 全文検索のみ（ベクトル検索未対応）
- 自然言語クエリの処理は限定的

**評価**: ⭐⭐⭐⭐ (軽量・高速だがベクトル検索なし)

---

### 3. Qdrant

**概要**: ベクトルデータベース（Rust製）

**メリット**:
- **ベクトル検索特化**（LLMとの親和性が高い）
- オープンソース（Apache 2.0）
- REST API + gRPC対応
- Dockerでのローカル構築が容易
- ハイブリッド検索（ベクトル + フィルタ）対応
- 軽量（~200MB Dockerイメージ）

**デメリット**:
- 全文検索は限定的（メタデータフィルタリングのみ）
- ベクトル埋め込みが必要（LLMモデルが必要）

**ローカル構築**:
```yaml
# docker-compose.yml
qdrant:
  image: qdrant/qdrant:latest
  ports:
    - "6333:6333"
    - "6334:6334"
  volumes:
    - qdrant_storage:/qdrant/storage
```

**LLM連携**: 
- **最適**: ベクトル検索に特化
- 埋め込みモデル（例: sentence-transformers）と組み合わせ
- 自然言語クエリ → ベクトル → 類似度検索

**評価**: ⭐⭐⭐⭐⭐ (LLM検索に最適)

---

## 比較表

| ライブラリ | 全文検索 | ベクトル検索 | Go統合 | ローカル構築 | リソース | LLM連携 |
|------------|---------|-------------|--------|-------------|----------|---------|
| Elasticsearch | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ (有料) | ⭐⭐⭐ | ⭐⭐⭐⭐ | 重い | ⭐⭐⭐ |
| Meilisearch | ⭐⭐⭐⭐⭐ | ❌ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | 軽量 | ⭐⭐ |
| Qdrant | ⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | 軽量 | ⭐⭐⭐⭐⭐ |
| PostgreSQL+pgvector | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 中 | ⭐⭐⭐⭐ |
| SQLite FTS | ⭐⭐⭐ | ❌ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | 軽量 | ⭐⭐ |

## 推奨候補（3つ）

### 🥇 推奨1: Qdrant + 埋め込みモデル

**理由**:
- LLMベースの自然言語検索に最適
- ローカル構築が容易
- 軽量
- オープンソース

**実装イメージ**:
```
OCR結果テキスト
  ↓
埋め込みモデル（sentence-transformers等）
  ↓
ベクトル化
  ↓
Qdrantに保存
  ↓
自然言語クエリ → ベクトル → 類似度検索
```

**課題**:
- 埋め込みモデルの実行環境が必要（Python/Docker）

---

### 🥈 推奨2: PostgreSQL + pgvector + FTS

**理由**:
- 全文検索とベクトル検索の両方に対応
- 既存インフラの拡張として活用可能
- トランザクション性
- 成熟した技術

**実装イメージ**:
```
OCR結果テキスト
  ↓
PostgreSQLに保存
  - FTS: 全文検索用
  - pgvector: ベクトル検索用
  ↓
ハイブリッド検索（全文 + ベクトル）
```

---

### 🥉 推奨3: SQLite FTS（現状維持）

**理由**:
- 既にSQLiteを使用中
- 追加依存関係なし
- 簡単に統合可能
- 軽量

**制限**:
- ベクトル検索は未対応
- LLM連携は限定的

---

## 実装推奨順序

### Phase 1: 全文検索（即座に実装可能）
1. **SQLite FTS** を活用して基本的な全文検索を実装
2. 既存の`ocr_results`テーブルにFTSインデックスを追加

### Phase 2: ベクトル検索（LLM連携）
1. **Qdrant** をDocker Composeに追加
2. 埋め込みモデルを実行するサービスを追加（Python/Go）
3. OCR結果をベクトル化してQdrantに保存
4. 自然言語クエリからベクトル検索を実装

### Phase 3: ハイブリッド検索（最適化）
1. 全文検索とベクトル検索を組み合わせ
2. スコアリングの最適化

## 検証項目

各候補について以下を検証:

1. **ローカル構築の容易さ**: Docker Composeでの起動
2. **Go言語からの利用**: ライブラリ/APIクライアントの有無
3. **パフォーマンス**: 1000件のOCR結果での検索速度
4. **LLM連携**: 埋め込みモデルとの統合のしやすさ
5. **リソース消費**: メモリ・CPU使用量

## 次のステップ

1. SQLite FTSで基本的な全文検索を実装（即座に可能）
2. QdrantでのPoC（Proof of Concept）を実施
3. 埋め込みモデルの選定と統合方法の検討
4. 検索APIの設計と実装
